{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classifying Snakes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook demonstrates how the `deepchecks` can help you evaluate a trained computer vision classification model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This scenario is something that happens to computer vision researchers in real life. Often when given a dataset in a production environment, an initial model will be trained and then evaluated against common benchmarks, only to achieve relative \"success\". Such benchmarks, when dealing with classification, are usually *accuracy*, *precision* and *recall*.\n",
    "The main problem is that those often hide serious problems with our data. In academic scenarios, the test set is sampled from the general set and that's completely fine. When working on real products, usually a \"test\" set should comprise of a much larger data distribution that isn't available to us until a product has actually been shipped to users. In such scenarios we can quickly come to the conclusion that our model isn't robust enough for various real-world cases which simply don't exist in our data. Examples can be a car dataset which doesn't include enough color variations, or a pedestrian detection model which is based only on images taken in a specific time of the day where's there's less (or more) sunlight.\n",
    "\n",
    "DeepCheck's computer vision toolkits consist of a RobustnessCheck tool which allows us to take a trained model and try to find such corner cases. In order to do that, it actually uses commonly (and less commonly) used image augmentations as ready-made image transformations.\n",
    "\n",
    "Image augmentations are a general group of image operators that modify a given image used as a data point when training a model. Augmenting a dataset effectively makes it larger by adding slightly different images. Not every image augmentation is suited for every task. A common example is horizontal shifts - which are often used, since most objects look the same when mirrored. This isn't true for a vertical shift, though, and because of that it's less commonly used as a default, despite the fact that it's actually quite useful for a *lot* of datasets (just about image taken from above).\n",
    "\n",
    "In the RobustnessCheck, as we'll soon see, we take a given trained model and run the test suite to visualize the image transformations (e.g. augmentations) the trained model is less \"robust\" to."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Note: The data was taken from Kaggle's [Pre-processed Snake Images](https://www.kaggle.com/sameeharahman/preprocessed-snake-images). It's license allows for commercial use. The model used for this example was trained by us and is also open for any sort of use. The train/val split is ours.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Installing requirements**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "!{sys.executable} -m pip install deepchecks --quiet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OK, let's take a look at the data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from deepchecks.vision.datasets.classification.snakes import load_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When dealing with images, a dataset usually contains tuples of *image and label*. Let's fetch the first image and take a close look."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(384, 384, 3)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_data()\n",
    "datapoint = dataset[0]\n",
    "len(datapoint)\n",
    "datapoint[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, images in the dataset are 384x384. Let's visualize the first two images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from skimage import io\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2)\n",
    "fig.add_trace(go.Image(z=dataset[0][0]), 1, 1)\n",
    "fig.add_trace(go.Image(z=dataset[1][0]), 1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that since we didn't define a *transformer* for the dataset yet, actual dataset output is of raw images. As we know, when working with DL models, the actual output of a dataset/dataloader is already processed to fit our architecture. Let's do that:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "transforms = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=256),\n",
    "    A.CenterCrop(height=224, width=224),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "dataset.transforms = transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try again:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 224, 224])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoint = dataset[0]\n",
    "len(datapoint)\n",
    "datapoint[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, so now we have the common 224x224 normalized center crop used in common CV applications. Note that so far we are using the regulat torch Dataset object without any added bells and whistles."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3477\n"
     ]
    },
    {
     "data": {
      "text/plain": "['class-0', 'class-1', 'class-2', 'class-3', 'class-4']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "dataset.classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, our dataset has 3477 images so far and 5 classes.\n",
    "The reason we have gathered here is to see how the DeepChecks library can help us to evaluate our model. Let's load the DeepChecks performance check."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Checking Model Performance with DeepChecks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "G1zhZmwL-k01",
    "GJJmoaYz-k02",
    "tZDsa0sO-k02",
    "ve3n-frP-k02",
    "EGBtWBTT-k04",
    "BqLC4vew-k04",
    "P0-Ls2D3-k05",
    "SLLZk0kM-k05",
    "yWYQY_bP-k05",
    "BHOSwVfR-k05",
    "-uUrOhM2-k05",
    "S_JqdY6n-k05",
    "qRxTfscP-k05"
   ],
   "name": "Copy of phishing_urls.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}