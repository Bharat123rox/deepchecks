{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classifying Snakes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook demonstrates how the `deepchecks` library can help computer vision researchers avoid some of the most common pitfalls in computer vision modeling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The scenarios outlined below are very common when working with imaging data. Often when given a dataset in a production environment, an initial model will be trained and then evaluated against common benchmarks, only to achieve relative \"success\". Such benchmarks, when dealing with classification, are usually *accuracy*, *precision* and *recall*.\n",
    "The main problem is that those often hide serious problems with our data. In academic scenarios, the test set is sampled from the general set and that's completely fine. When working on real products, usually a \"test\" set should comprise of a much larger data distribution that isn't available to us until a product has actually been shipped to users. In such scenarios we can quickly come to the conclusion that our model isn't robust enough for various real-world cases which simply don't exist in our data. Examples can be a car dataset which doesn't include enough color variations, or a pedestrian detection model which is based only on images taken in a specific time of the day where there's less (or more) sunlight.\n",
    "\n",
    "DeepCheck's computer vision toolkits consist of several \"Check\" classes which allow us to take both the model and the datasets used to train and evaluated it and try to find such reasons why it might fail (or already does)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Note: The data was taken from Kaggle's [Pre-processed Snake Images](https://www.kaggle.com/sameeharahman/preprocessed-snake-images). It's license allows for commercial use. The model used for this example was trained by us and is also open for any sort of use. The train/val split is ours.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Installing requirements**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "!{sys.executable} -m pip install deepchecks --quiet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OK, let's take a look at the data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from deepchecks.vision.datasets.classification.snakes import load_val_data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When dealing with images, a dataset usually contains tuples of *image and label*. Let's fetch the first image and take a close look."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "(384, 384, 3)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_val_data()\n",
    "datapoint = dataset[0]\n",
    "len(datapoint)\n",
    "datapoint[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, images in the dataset are 384x384. Let's visualize the first two images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from skimage import io\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2)\n",
    "fig.add_trace(go.Image(z=dataset[0][0]), 1, 1)\n",
    "fig.add_trace(go.Image(z=dataset[1][0]), 1, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that since we didn't define a *transformer* for the dataset yet, actual dataset output is of raw images. As we know, when working with DL models, the actual output of a dataset/dataloader is already processed to fit our architecture. Let's do that:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "transforms = A.Compose([\n",
    "    A.SmallestMaxSize(max_size=256),\n",
    "    A.CenterCrop(height=224, width=224),\n",
    "    A.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "dataset.transforms = transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try again:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 224, 224])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoint = dataset[0]\n",
    "len(datapoint)\n",
    "datapoint[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, so now we have the common 224x224 normalized center crop used in common CV applications. Note that so far we are using the regulat torch Dataset object without any added bells and whistles."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3477\n"
     ]
    },
    {
     "data": {
      "text/plain": "['class-0', 'class-1', 'class-2', 'class-3', 'class-4']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "dataset.classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So, our dataset has 3477 images so far and 5 classes.\n",
    "The reason we have gathered here is to see how the DeepChecks library can help us to evaluate and improve computer vision modeling.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computer Vision Modeling Using DeepChecks\n",
    "We will now take a closer look at the various \"Checks\" that are available in the DeepChecks library for computer vision researchers.\n",
    "\n",
    "## Some Preperations\n",
    "Let's load the train dataset in a similar fashion to the one applied to the validation dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nirbenzvi/code/DeepChecks/.venv/lib/python3.9/site-packages/torchvision/io/image.py:11: UserWarning:\n",
      "\n",
      "Failed to load image Python extension: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deepchecks.vision.datasets.classification.snakes import load_train_data\n",
    "# Note that we are using \"transformed\" to already apply the required transformations to images\n",
    "train_dataset = load_train_data(transformed=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both datasets need to be wrapped by PyTorch's standard `torch.utils.Dataloader to support faster completion of tests."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, num_workers=4)\n",
    "val_dataloader = DataLoader(dataset, batch_size=4, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "DeepCheck's code uses a wrapper around PyTorch's dataloader, which hides away a lot of the validity checks that are performed on a dataset. Let's use those:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from deepchecks.vision import VisionData\n",
    "\n",
    "train_ds = VisionData(train_dataloader)\n",
    "val_ds = VisionData(val_dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loading and creation of both `VisionData` objects takes a second because of so many checks that are performed behing the scenes here!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train Test Label Drift\n",
    "DeepCheck's `TrainTestLabelDrift` checks for variations between the data used to train the model and that used to test it.\n",
    "Often times when working on computer vision models one might run into label disparity between train and test/validation datasets. This can obviously hurt model performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from deepchecks.vision.checks import TrainTestLabelDrift\n",
    "\n",
    "check = TrainTestLabelDrift()\n",
    "check_result = check.run(train_ds, val_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Performance\n",
    "DeepCheck's `PerformanceCheck` tests the model for various common metrics. This might sound simple but it's actually so common for researchers to simply take a \"good\" snapshot from a trained model and move it into staging without properly evaluating it. Most classification models are only evaluated for accuracy during training, and the DeepCheck library allows us to quickly and effortlessly check for other popular (and maybe less popular) metrics."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from deepchecks.vision.checks import ClassPerformance\n",
    "from deepchecks.vision.datasets.classification.snakes import load_model\n",
    "\n",
    "# load the trained snakes model\n",
    "model = load_model()\n",
    "check = ClassPerformance()\n",
    "check_result = check.run(train_ds, val_ds, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Robustness Checks\n",
    "\n",
    "DeepCheck's `ModelRobustness` checks that a model can cope with real life shifts in data distribution. When dealing with imaes that simply means that images have changed visually compared to those used to train the model.\n",
    "In order to do that, the library actually uses commonly (and less commonly) used image augmentations as ready-made image transformations.\n",
    "\n",
    "Image augmentations are a general group of image operators that modify a given image used as a data point when training a model. Augmenting a dataset effectively makes it larger by adding slightly different images. Not every image augmentation is suited for every task. A common example is horizontal shifts - which are often used, since most objects look the same when mirrored. This isn't true for a vertical shift, though, and because of that it's less commonly used as a default, despite the fact that it's actually quite useful for a *lot* of datasets (just about image taken from above)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from deepchecks.vision.checks.performance.robustness_report import RobustnessReport\n",
    "\n",
    "check = RobustnessReport()\n",
    "check_result = check.run(dataset, model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "G1zhZmwL-k01",
    "GJJmoaYz-k02",
    "tZDsa0sO-k02",
    "ve3n-frP-k02",
    "EGBtWBTT-k04",
    "BqLC4vew-k04",
    "P0-Ls2D3-k05",
    "SLLZk0kM-k05",
    "yWYQY_bP-k05",
    "BHOSwVfR-k05",
    "-uUrOhM2-k05",
    "S_JqdY6n-k05",
    "qRxTfscP-k05"
   ],
   "name": "Copy of phishing_urls.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 1
}