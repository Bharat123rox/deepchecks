{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918574f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd8b5e",
   "metadata": {},
   "source": [
    "## Login W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00589a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepchecks\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0f5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"dc-wandb-webinar\"\n",
    "ENTITY_NAME = \"deepchecks\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341205d9",
   "metadata": {},
   "source": [
    "## Load Avocado Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82fa80b",
   "metadata": {},
   "source": [
    "### Get data and save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b020ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import datasets\n",
    "import avocado_dataset_utils\n",
    "\n",
    "data = datasets.regression.avocado.load_data(data_format='DataFrame', as_train_test=False)\n",
    "\n",
    "# a bit preprocessing\n",
    "train_df, test_df = avocado_dataset_utils.get_train_test_df_from_raw(data)\n",
    "label_col_name = 'IsExpensive'\n",
    "categorical_features = ['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f9f09",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "So checks have metadata context (label, categorical feature, date column if exists, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cfc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n",
    "\n",
    "train_ds = Dataset(train_df, label=label_col_name, cat_features=categorical_features)\n",
    "test_ds = Dataset(test_df, label=label_col_name, cat_features=categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48638ab",
   "metadata": {},
   "source": [
    "## Check Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8e0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shir\\Downloads\\deepchecks-wandb-webinar\\venv\\lib\\site-packages\\deepchecks\\suites.py:21: DeprecationWarning:\n",
      "\n",
      "Ability to import tabular suites from the `deepchecks.suites` is deprecated, please import from `deepchecks.tabular.suites` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Single Dataset Integrity Suite:   0%|          | 0/8 [00:00<?, ? Check/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b60cdcfa654588b9aeaa32877cc777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n        <h1 id=\"summary_BGV45\">Single Dataset Integrity Suite</h1>\\n        <p>\\n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import single_dataset_integrity\n",
    "\n",
    "integ_suite_results = single_dataset_integrity().run(train_ds)\n",
    "integ_suite_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8b13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Shir\\Downloads\\deepchecks-wandb-webinar\\wandb\\run-20220407_124553-2clhd576</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deepchecks/demo/runs/2clhd576\" target=\"_blank\">init_data</a></strong> to <a href=\"https://wandb.ai/deepchecks/demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Single Dataset Integrity Suite:   0%|          | 0/8 [00:00<?, ? Result/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">init_data</strong>: <a href=\"https://wandb.ai/deepchecks/demo/runs/2clhd576\" target=\"_blank\">https://wandb.ai/deepchecks/demo/runs/2clhd576</a><br/>Synced 4 W&B file(s), 16 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220407_124553-2clhd576\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "integ_suite_results.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"initial-data-integrity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1cc593",
   "metadata": {},
   "source": [
    "### add noise to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd69fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data duplicates, ambiguous labels, string mismatch, ...\n",
    "dirty_train_df = avocado_dataset_utils.add_dirty_data_to_single_df(train_df)\n",
    "dirty_train_ds = Dataset(dirty_train_df, cat_features = categorical_features, label = label_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5a19f",
   "metadata": {},
   "source": [
    "### Rerun integrity suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf9013d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Single Dataset Integrity Suite:   0%|          | 0/8 [00:00<?, ? Check/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcece5ecc1814d5aa80d09877226c591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n        <h1 id=\"summary_AL4S8\">Single Dataset Integrity Suite</h1>\\n        <p>\\n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirty_integ_suite_results = single_dataset_integrity().run(dirty_train_ds)\n",
    "dirty_integ_suite_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787a77e",
   "metadata": {},
   "source": [
    "#### Log suite results to w&b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cced25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Shir\\Downloads\\deepchecks-wandb-webinar\\wandb\\run-20220407_124742-2jp2mvgd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deepchecks/demo/runs/2jp2mvgd\" target=\"_blank\">dirty_data</a></strong> to <a href=\"https://wandb.ai/deepchecks/demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Single Dataset Integrity Suite:   0%|          | 0/8 [00:00<?, ? Result/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dirty_data</strong>: <a href=\"https://wandb.ai/deepchecks/demo/runs/2jp2mvgd\" target=\"_blank\">https://wandb.ai/deepchecks/demo/runs/2jp2mvgd</a><br/>Synced 4 W&B file(s), 20 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220407_124742-2jp2mvgd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirty_integ_suite_results.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"dirty-data-integrity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d147b9",
   "metadata": {},
   "source": [
    "#### Now lets log (in a new run) specifically checks that didn't pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792f9cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Shir\\Downloads\\deepchecks-wandb-webinar\\wandb\\run-20220407_125653-1249gens</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deepchecks/demo/runs/1249gens\" target=\"_blank\">dirty_data_not_pass</a></strong> to <a href=\"https://wandb.ai/deepchecks/demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Single Value in Column Didn't Pass, logging to wandb\n",
      "Check String Mismatch Didn't Pass, logging to wandb\n",
      "Check Data Duplicates Didn't Pass, logging to wandb\n",
      "Check Label Ambiguity Didn't Pass, logging to wandb\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dirty_data_not_pass</strong>: <a href=\"https://wandb.ai/deepchecks/demo/runs/1249gens\" target=\"_blank\">https://wandb.ai/deepchecks/demo/runs/1249gens</a><br/>Synced 5 W&B file(s), 12 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220407_125653-1249gens\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult\n",
    "\n",
    "# we want to have them together in one run, so lets init the run:\n",
    "wandb.init(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"integrity-not-passing-checks\")\n",
    "\n",
    "for check_result in dirty_integ_suite_results.results:\n",
    "    if type(check_result) is CheckResult:\n",
    "        if not check_result.passed_conditions():\n",
    "            print(\"Check **{}** didn't Pass. Saving result to wandb\".format(check_result.get_header()))\n",
    "            # Save here to the same run:\n",
    "            check_result.to_wandb(dedicated_run = False)\n",
    "            \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812acdf",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1810f",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_ohe_columns = avocado_dataset_utils.ohe_for_type_column(train_df)\n",
    "test_df, _ = avocado_dataset_utils.ohe_for_type_column(test_df, train_ohe_columns)\n",
    "\n",
    "train_ds = Dataset(train_df, label=label_col_name, cat_features=[])\n",
    "test_ds = Dataset(test_df, label=label_col_name, cat_features=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8663a73",
   "metadata": {},
   "source": [
    "### Fit & Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ed437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# fit\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(train_ds.features_columns, train_ds.label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde341b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred_train = rf_clf.predict(train_ds.features_columns)\n",
    "y_pred_test = rf_clf.predict(test_ds.features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6389c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# minimal metrics check - model accuracty\n",
    "train_acc = accuracy_score(train_ds.label_col, y_pred_train)\n",
    "test_acc = accuracy_score(test_ds.label_col, y_pred_test)\n",
    "print(\"Train accuracy: {}, Test accuracy: {}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df487c2",
   "metadata": {},
   "source": [
    "### Analyze Splits and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820702be",
   "metadata": {},
   "source": [
    "#### Model Evaluation Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5456e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import model_evaluation\n",
    "\n",
    "me_suite_result = model_evaluation().run(train_ds, test_ds, rf_clf)\n",
    "me_suite_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e807e",
   "metadata": {},
   "source": [
    "#### Run performance report and log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import PerformanceReport\n",
    "\n",
    "perf_result = PerformanceReport().run(train_ds, test_ds, rf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eab8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show output value\n",
    "perf_result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show \n",
    "perf_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f91a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_result.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"performance-report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900abb8",
   "metadata": {},
   "source": [
    "#### Inspect Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05e614",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import train_test_validation\n",
    "\n",
    "train_test_validation().run(train_ds, test_ds, rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c7ccc",
   "metadata": {},
   "source": [
    "### Run specific checks - separately or in a suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6170ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import ConfusionMatrixReport, CalibrationScore, DataDuplicates\n",
    "from deepchecks.tabular import Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28530fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_suite = Suite('Custom Evaluation', CalibrationScore(), ConfusionMatrixReport())\n",
    "suite_res = custom_suite.run(train_ds, test_ds, rf_clf)\n",
    "suite_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_res.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"custom-model-eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251bee3c",
   "metadata": {},
   "source": [
    "## Happy, Evaluated and Valid Models and Data...\n",
    "\n",
    "The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487524b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original utils (moved to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da07ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # minimal preprocessing and data splitting - move to utils file....\n",
    "# import pandas as pd\n",
    "\n",
    "# def get_classification_label_from_avg_price(df):\n",
    "#     EXPENSIVE_AVOCADO_AVG_PRICE = 1.6\n",
    "#     df['IsExpensive'] = df['AveragePrice'] > EXPENSIVE_AVOCADO_AVG_PRICE\n",
    "#     df = df.drop(columns='AveragePrice')\n",
    "#     return df\n",
    "\n",
    "# def drop_unused_columns(df):\n",
    "#     df = df.drop(columns=['Unnamed: 0'])\n",
    "#     df = df.drop(columns=['region', 'Date'])\n",
    "#     return df\n",
    "\n",
    "# def organize_df(df):\n",
    "#     df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "#     df = get_classification_label_from_avg_price(df)\n",
    "#     df = drop_unused_columns(df)\n",
    "#     return df\n",
    "\n",
    "# def ohe_for_type_column(df, org_dummies_columns=None):\n",
    "#     # there are only two values for type, getting dummies for all dataframe together\n",
    "#     generated_columns = None\n",
    "#     if 'type' in df.columns:\n",
    "#         dummies = pd.get_dummies(df['type'])\n",
    "#         generated_dummy_columns = dummies.columns\n",
    "#         if org_dummies_columns is not None:\n",
    "#             columns_only_in_org = set(org_dummies_columns) - set(generated_dummy_columns)\n",
    "#             if columns_only_in_org:\n",
    "#                 # append also columns existant in previous\n",
    "#                 dummies[list(columns_only_in_org)] = 0\n",
    "#             columns_only_in_result = set(generated_dummy_columns) - set(org_dummies_columns)\n",
    "#             if columns_only_in_result:\n",
    "#                 dummies = dummies.drop(columns=list(columns_only_in_result))\n",
    "#             # set order to be similar\n",
    "#             dummies = dummies[org_dummies_columns]\n",
    "#         df = pd.concat([df, dummies], axis=1)\n",
    "#         df = df.drop(columns='type')\n",
    "#     return df, generated_dummy_columns\n",
    "\n",
    "# def get_train_test_df_from_raw(df, train_frac=0.7):\n",
    "#     # nothing here special to train/test so doing it together\n",
    "#     df = organize_df(df)\n",
    "    \n",
    "#     # data is chronological, so split should be accordingly\n",
    "#     train_samples = int(len(df)*train_frac)\n",
    "#     train_df = df[:train_samples]\n",
    "#     test_df = df[train_samples:]\n",
    "#     return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260291de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make data dirty - move to utils file...\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # def add_something_bad_to_model():\n",
    "# #     # how?? dunno... but can show comparison...\n",
    "# #     pass\n",
    "\n",
    "# def add_organic_string_mismatch(df):\n",
    "#     # copying since changes are inplace. returning df for consistency\n",
    "#     df_dirty = df.copy(deep=True)\n",
    "#     df_dirty.loc[df[df['type'] == 'organic'].sample(frac=0.18).index,'type'] = 'Organic'\n",
    "#     df_dirty.loc[df[df['type'] == 'organic'].sample(frac=0.01).index,'type'] = 'ORGANIC'\n",
    "#     return df_dirty\n",
    "\n",
    "\n",
    "# def add_duplicates_with_shuffled_labels(df, label_col_name='IsExpensive'):\n",
    "#     frac_to_duplicate = 0.36\n",
    "#     labels = df[label_col_name]\n",
    "#     duplicated_samples = df.sample(frac=frac_to_duplicate)\n",
    "#     # shuffle labels, will likely add ambiguity\n",
    "#     duplicated_samples[label_col_name] = duplicated_samples[label_col_name].sample(frac=1, ignore_index=True)\n",
    "#     df_dup_labels = pd.concat([df, duplicated_samples], ignore_index=True)\n",
    "#     return df_dup_labels\n",
    "    \n",
    "\n",
    "# def add_data_duplicates(df, frac_to_duplicate=0.156):\n",
    "#     duplicated_samples = df.sample(frac=frac_to_duplicate)\n",
    "#     df_dup = pd.concat([df, duplicated_samples], axis=0, ignore_index=True)\n",
    "#     return df_dup\n",
    "\n",
    "    \n",
    "# def add_single_value(df):\n",
    "#     new_column_df = df.copy(deep=True)\n",
    "#     new_column_df['Is Ripe'] = True\n",
    "#     return new_column_df\n",
    "\n",
    "\n",
    "# def add_test_sample_leakage_to_train(train_df, test_df):\n",
    "#     train_df = pd.concat([test_df.sample(frac=0.03), train_df], ignore_index=True)\n",
    "#     return train_df\n",
    "\n",
    "\n",
    "# def add_dirty_data_to_single_df(df):\n",
    "#     df = add_organic_string_mismatch(df)\n",
    "#     df = add_duplicates_with_shuffled_labels(df)\n",
    "#     df = add_data_duplicates(df)\n",
    "#     df = add_single_value(df)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def add_regression_drift_to_test_labels(test_df, label_col_name = 'AveragePrice'):\n",
    "#     test_df[label_col_name] = test_df[label_col_name] + np.random.rand(test_df.shape[0])*test_df[label_col_name].std()*4\n",
    "#     return test_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
