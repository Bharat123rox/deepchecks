{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "918574f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd8b5e",
   "metadata": {},
   "source": [
    "## Login W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00589a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepchecks\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0f5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"dc-wandb-webinar\"\n",
    "ENTITY_NAME = \"deepchecks\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341205d9",
   "metadata": {},
   "source": [
    "## Load Avocado Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82fa80b",
   "metadata": {},
   "source": [
    "### Get data and save metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b020ea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import datasets\n",
    "import avocado_dataset_utils\n",
    "\n",
    "data = datasets.regression.avocado.load_data(data_format='DataFrame', as_train_test=False)\n",
    "\n",
    "# a bit preprocessing\n",
    "train_df, test_df = avocado_dataset_utils.get_train_test_df_from_raw(data)\n",
    "label_col_name = 'IsExpensive'\n",
    "categorical_features = ['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34f9f09",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "So checks have metadata context (label, categorical feature, date column if exists, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3cfc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n",
    "\n",
    "train_ds = Dataset(train_df, label=label_col_name, cat_features=categorical_features)\n",
    "test_ds = Dataset(test_df, label=label_col_name, cat_features=categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48638ab",
   "metadata": {},
   "source": [
    "## Check Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8e0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shir\\Downloads\\deepchecks-wandb-webinar\\venv\\lib\\site-packages\\deepchecks\\suites.py:21: DeprecationWarning:\n",
      "\n",
      "Ability to import tabular suites from the `deepchecks.suites` is deprecated, please import from `deepchecks.tabular.suites` instead\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Single Dataset Integrity Suite:   0%|          | 0/8 [00:00<?, ? Check/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b60cdcfa654588b9aeaa32877cc777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n        <h1 id=\"summary_BGV45\">Single Dataset Integrity Suite</h1>\\n        <p>\\n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.tabular.suites import single_dataset_integrity\n",
    "\n",
    "integ_suite_results = single_dataset_integrity().run(train_ds)\n",
    "integ_suite_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c8b13d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Shir\\Downloads\\deepchecks-wandb-webinar\\wandb\\run-20220407_124553-2clhd576</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deepchecks/demo/runs/2clhd576\" target=\"_blank\">init_data</a></strong> to <a href=\"https://wandb.ai/deepchecks/demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Single Dataset Integrity Suite:   0%|          | 0/8 [00:00<?, ? Result/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">init_data</strong>: <a href=\"https://wandb.ai/deepchecks/demo/runs/2clhd576\" target=\"_blank\">https://wandb.ai/deepchecks/demo/runs/2clhd576</a><br/>Synced 4 W&B file(s), 16 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220407_124553-2clhd576\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "integ_suite_results.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"initial-data-integrity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1cc593",
   "metadata": {},
   "source": [
    "### add noise to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd69fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data duplicates, ambiguous labels, string mismatch, ...\n",
    "dirty_train_df = avocado_dataset_utils.add_dirty_data_to_single_df(train_df)\n",
    "dirty_train_ds = Dataset(dirty_train_df, cat_features = categorical_features, label = label_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5a19f",
   "metadata": {},
   "source": [
    "### Rerun integrity suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf9013d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Single Dataset Integrity Suite:   0%|          | 0/8 [00:00<?, ? Check/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcece5ecc1814d5aa80d09877226c591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n        <h1 id=\"summary_AL4S8\">Single Dataset Integrity Suite</h1>\\n        <p>\\n…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirty_integ_suite_results = single_dataset_integrity().run(dirty_train_ds)\n",
    "dirty_integ_suite_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa0701",
   "metadata": {},
   "source": [
    "#### Log suite results to w&b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cced25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Shir\\Downloads\\deepchecks-wandb-webinar\\wandb\\run-20220407_124742-2jp2mvgd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deepchecks/demo/runs/2jp2mvgd\" target=\"_blank\">dirty_data</a></strong> to <a href=\"https://wandb.ai/deepchecks/demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Single Dataset Integrity Suite:   0%|          | 0/8 [00:00<?, ? Result/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.018 MB of 0.018 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dirty_data</strong>: <a href=\"https://wandb.ai/deepchecks/demo/runs/2jp2mvgd\" target=\"_blank\">https://wandb.ai/deepchecks/demo/runs/2jp2mvgd</a><br/>Synced 4 W&B file(s), 20 media file(s), 20 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220407_124742-2jp2mvgd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dirty_integ_suite_results.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"dirty-data-integrity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10f6028",
   "metadata": {},
   "source": [
    "#### Now lets log (in a new run) specifically checks that didn't pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792f9cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\Shir\\Downloads\\deepchecks-wandb-webinar\\wandb\\run-20220407_125653-1249gens</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/deepchecks/demo/runs/1249gens\" target=\"_blank\">dirty_data_not_pass</a></strong> to <a href=\"https://wandb.ai/deepchecks/demo\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Single Value in Column Didn't Pass, logging to wandb\n",
      "Check String Mismatch Didn't Pass, logging to wandb\n",
      "Check Data Duplicates Didn't Pass, logging to wandb\n",
      "Check Label Ambiguity Didn't Pass, logging to wandb\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dirty_data_not_pass</strong>: <a href=\"https://wandb.ai/deepchecks/demo/runs/1249gens\" target=\"_blank\">https://wandb.ai/deepchecks/demo/runs/1249gens</a><br/>Synced 5 W&B file(s), 12 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220407_125653-1249gens\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.core import CheckResult\n",
    "\n",
    "# we want to have them together in one run, so lets init the run:\n",
    "wandb.init(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"integrity-not-passing-checks\")\n",
    "\n",
    "for check_result in dirty_integ_suite_results.results:\n",
    "    if type(check_result) is CheckResult:\n",
    "        if not check_result.passed_conditions():\n",
    "            print(\"Check **{}** didn't Pass. Saving result to wandb\".format(check_result.get_header()))\n",
    "            # Save here to the same run:\n",
    "            check_result.to_wandb(dedicated_run = False)\n",
    "            \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812acdf",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1810f",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, train_ohe_columns = avocado_dataset_utils.ohe_for_type_column(train_df)\n",
    "test_df, _ = avocado_dataset_utils.ohe_for_type_column(test_df, train_ohe_columns)\n",
    "\n",
    "train_ds = Dataset(train_df, label=label_col_name, cat_features=[])\n",
    "test_ds = Dataset(test_df, label=label_col_name, cat_features=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8663a73",
   "metadata": {},
   "source": [
    "### Fit & Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ed437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# fit\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(train_ds.features_columns, train_ds.label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde341b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred_train = rf_clf.predict(train_ds.features_columns)\n",
    "y_pred_test = rf_clf.predict(test_ds.features_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6389c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# minimal metrics check - model accuracty\n",
    "train_acc = accuracy_score(train_ds.label_col, y_pred_train)\n",
    "test_acc = accuracy_score(test_ds.label_col, y_pred_test)\n",
    "print(\"Train accuracy: {}, Test accuracy: {}\".format(train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df487c2",
   "metadata": {},
   "source": [
    "### Analyze Splits and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820702be",
   "metadata": {},
   "source": [
    "#### Model Evaluation Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5456e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import model_evaluation\n",
    "\n",
    "me_suite_result = model_evaluation().run(train_ds, test_ds, rf_clf)\n",
    "me_suite_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e807e",
   "metadata": {},
   "source": [
    "#### Run performance report and log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be12d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import PerformanceReport\n",
    "\n",
    "perf_result = PerformanceReport().run(train_ds, test_ds, rf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eab8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show output value\n",
    "perf_result.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show \n",
    "perf_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f91a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_result.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"performance-report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900abb8",
   "metadata": {},
   "source": [
    "#### Inspect Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05e614",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import train_test_validation\n",
    "\n",
    "train_test_validation().run(train_ds, test_ds, rf_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03c7ccc",
   "metadata": {},
   "source": [
    "### Run specific checks - separately or in a suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6170ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import ConfusionMatrixReport, CalibrationScore, DataDuplicates\n",
    "from deepchecks.tabular import Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28530fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_suite = Suite('Custom Evaluation', CalibrationScore(), ConfusionMatrixReport())\n",
    "suite_res = custom_suite.run(train_ds, test_ds, rf_clf)\n",
    "suite_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite_res.to_wandb(project=PROJECT_NAME, entity=ENTITY_NAME, name=\"custom-model-eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251bee3c",
   "metadata": {},
   "source": [
    "## Happy, Evaluated and Valid Models and Data...\n",
    "\n",
    "The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
